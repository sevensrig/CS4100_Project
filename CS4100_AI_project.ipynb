{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tech Trends News Agent\n",
        "\n",
        "An AI-powered news analysis system that retrieves technology articles, enables semantic search, generates conversational answers using a local LLM, and visualizes trending topics.\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "1. Click the `.ipynb` notebook file in this repository\n",
        "2. Click **Open in Colab** at the top\n",
        "3. Set runtime to **GPU** (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
        "4. Run all cells (Runtime ‚Üí Run all)\n",
        "5. Click the ngrok URL when it appears to open the app\n",
        "\n",
        "---\n",
        "\n",
        "## Setup and Run Instructions\n",
        "\n",
        "### 1. Prerequisites\n",
        "\n",
        "- Google account (for Colab)\n",
        "- That's it! Everything else runs in the cloud.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Run in Google Colab\n",
        "\n",
        "1. Click the notebook file (`Copy_of_12_4_New_AI_project.ipynb`) above\n",
        "2. Click **Open in Colab** button at the top of the file\n",
        "3. Go to **Runtime ‚Üí Change runtime type ‚Üí Select GPU (T4)**\n",
        "4. Run all cells in order (Runtime ‚Üí Run all)\n",
        "5. Wait for the ngrok URL to appear (~60 seconds for model loading)\n",
        "6. Click the ngrok URL to open the Streamlit app\n",
        "\n",
        "---\n",
        "\n",
        "### 3. API Key\n",
        "\n",
        "You **do not need to create your own NewsAPI key**. A working API key is already included in the project configuration. The agent will automatically use it when retrieving articles.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Project Structure\n",
        "\n",
        "```\n",
        "tech-trends-agent/\n",
        "‚îú‚îÄ‚îÄ app.py                 # Streamlit web interface\n",
        "‚îú‚îÄ‚îÄ fetch_news.py          # NewsAPI article retrieval\n",
        "‚îú‚îÄ‚îÄ preprocess.py          # NLTK tokenization and preprocessing\n",
        "‚îú‚îÄ‚îÄ search_articles.py     # TF-IDF search and topic extraction\n",
        "‚îú‚îÄ‚îÄ llm_interface.py       # Qwen2.5-1.5B model wrapper\n",
        "‚îú‚îÄ‚îÄ data/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Raw JSON from NewsAPI\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ processed/         # Preprocessed article corpus\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Using the App\n",
        "\n",
        "#### Chat Tab\n",
        "1. Click **\"Fetch Articles\"** in the sidebar to load latest technology news\n",
        "2. Type a question (e.g., \"What are the latest AI trends?\") or click a suggestion\n",
        "3. View the LLM-generated answer with expandable source citations\n",
        "\n",
        "#### Topic Visualization Tab\n",
        "1. After fetching articles, switch to the **\"Topic Visualization\"** tab\n",
        "2. View bar chart of top 5 discovered topics\n",
        "3. Expand each topic to see key terms, sample articles, and sub-trends\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Output\n",
        "\n",
        "- **Chat Responses**: Contextual answers displayed in the chat interface with source links\n",
        "- **Topic Clusters**: Automatically extracted from article corpus using TF-IDF and co-occurrence analysis\n",
        "- **Sub-Trends**: Relevance-scored sub-topics within each main topic cluster\n",
        "- **Metrics**: Total articles, categorized count, and coverage percentage\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Notes for Graders\n",
        "\n",
        "- **Easiest path**: Run the Colab notebook with GPU runtime enabled\n",
        "- The API key is pre-configured ‚Äî no setup required for NewsAPI\n",
        "- Model loading takes ~60 seconds on first run (downloading ~3GB)\n",
        "- Once loaded, queries are answered in 2-5 seconds\n",
        "- If NewsAPI rate limits are hit, previously fetched articles are cached in `data/processed/`\n",
        "- The app automatically loads cached articles if available\n",
        "\n",
        "#### To Test:\n",
        "1. Run all cells in the notebook\n",
        "2. Click \"Fetch Articles\" to retrieve latest news\n",
        "3. Ask questions in the chat interface\n",
        "4. Check the Topic Visualization tab for trend analysis\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Technical Details\n",
        "\n",
        "- **LLM**: Qwen2.5-1.5B-Instruct\n",
        "- **Search**: TF-IDF with cosine similarity\n",
        "- **Preprocessing**: NLTK (tokenize, stopwords, lemmatize)\n",
        "- **Topic Extraction**: Co-occurrence clustering\n",
        "- **Frontend**: Streamlit\n",
        "- **Data Source**: NewsAPI (technology headlines)\n",
        "- **GPU Support**: CUDA (float16 precision)\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Troubleshooting\n",
        "\n",
        "- **\"No articles loaded\"**: Click \"Fetch Articles\" in sidebar\n",
        "- **Model loading slow**: Ensure GPU runtime is enabled in Colab\n",
        "- **ngrok URL not working**: Re-run the last cell to generate new tunnel\n",
        "- **NewsAPI error**: Rate limit reached; wait or use cached data\n",
        "- **CUDA out of memory**: Restart runtime and try again"
      ],
      "metadata": {
        "id": "n8NnyqNNiTh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-jKFs2Y1xqf",
        "outputId": "13d82b01-6905-48c5-f794-1bdf21549f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec  9 00:58:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0             35W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Directories created\n"
          ]
        }
      ],
      "source": [
        "# ===CELL 1: Setup and Install Dependencies===\n",
        "#@title 1. Check GPU & Install Dependencies\n",
        "!nvidia-smi\n",
        "!pip install -q transformers torch accelerate nltk streamlit pyngrok newsapi-python\n",
        "\n",
        "import os\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "print(\"Directories created\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Create fetch_news.py\n",
        "%%writefile fetch_news.py\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "TECH_DOMAINS = \"techcrunch.com,theverge.com,wired.com,arstechnica.com,engadget.com,venturebeat.com,techradar.com,gizmodo.com,reuters.com,forbes.com,zdnet.com,cnet.com,mashable.com,thenextweb.com,businessinsider.com,bloomberg.com,bbc.com,theguardian.com,nytimes.com,washingtonpost.com,apnews.com,cnbc.com,axios.com,protocol.com,techmeme.com,9to5mac.com,9to5google.com,androidcentral.com,macrumors.com,tomshardware.com,pcmag.com,pcworld.com,computerworld.com,infoworld.com,theregister.com,siliconangle.com,techspot.com,digitaltrends.com,tomsguide.com,howtogeek.com,phonearena.com,gsmarena.com\"\n",
        "def fetch_news_articles(api_key: str, from_date: str, to_date: str,\n",
        "                        query: str = \"technology\",\n",
        "                        max_articles: int = 100) -> list:\n",
        "    newsapi = NewsApiClient(api_key=api_key)\n",
        "    all_articles = []\n",
        "    page = 1\n",
        "    page_size = min(100, max_articles)\n",
        "\n",
        "    print(f\"Fetching articles from {from_date} to {to_date}...\")\n",
        "\n",
        "    while len(all_articles) < max_articles:\n",
        "        try:\n",
        "            response = newsapi.get_everything(\n",
        "                domains=TECH_DOMAINS,\n",
        "                from_param=from_date,\n",
        "                to=to_date,\n",
        "                language='en',\n",
        "                sort_by='publishedAt',\n",
        "                page_size=page_size,\n",
        "                page=page\n",
        "            )\n",
        "            articles = response.get('articles', [])\n",
        "            print(f\"  Page {page}: found {len(articles)} articles\")\n",
        "            if not articles:\n",
        "                break\n",
        "            all_articles.extend(articles)\n",
        "            total_results = response.get('totalResults', 0)\n",
        "            print(f\"  Total available: {total_results}\")\n",
        "            if len(all_articles) >= total_results or len(all_articles) >= max_articles:\n",
        "                break\n",
        "            page += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    all_articles = all_articles[:max_articles]\n",
        "    os.makedirs(\"data/raw\", exist_ok=True)\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    raw_path = f\"data/raw/newsapi_{timestamp}.json\"\n",
        "    with open(raw_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_articles, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"[OK] Saved {len(all_articles)} articles to {raw_path}\")\n",
        "    return all_articles\n",
        "\n",
        "def get_date_range_options():\n",
        "    today = datetime.now()\n",
        "    return {\n",
        "        \"Last 24 hours\": (today - timedelta(days=1), today),\n",
        "        \"Last 3 days\": (today - timedelta(days=3), today),\n",
        "        \"Last 7 days\": (today - timedelta(days=7), today),\n",
        "        \"Last 14 days\": (today - timedelta(days=14), today),\n",
        "        \"Last 30 days\": (today - timedelta(days=30), today),\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRlddWbI1-sV",
        "outputId": "64656ab8-2c33-450b-aebc-1dfc5b91ec03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fetch_news.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===CELL 3: Create preprocess.py\n",
        "#@title 3. Create preprocess.py\n",
        "%%writefile preprocess.py\n",
        "import os, re, json\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text: return []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if re.match(r\"[a-zA-Z0-9]+\", t)]\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "def preprocess(articles=None):\n",
        "    if articles is None:\n",
        "        raw_dir = \"data/raw\"\n",
        "        files = sorted([f for f in os.listdir(raw_dir) if f.endswith(\".json\")], reverse=True)\n",
        "        if not files: raise FileNotFoundError(\"No raw files\")\n",
        "        with open(os.path.join(raw_dir, files[0]), \"r\", encoding=\"utf-8\") as f:\n",
        "            articles = json.load(f)\n",
        "\n",
        "    print(f\"Processing {len(articles)} articles...\")\n",
        "    processed = []\n",
        "\n",
        "    for idx, art in enumerate(articles, 1):\n",
        "        title = art.get(\"title\") or \"\"\n",
        "        desc = art.get(\"description\") or \"\"\n",
        "        content = re.sub(r\"\\[\\+\\d+ chars\\]$\", \"\", art.get(\"content\") or \"\").strip()\n",
        "        readable_text = f\"{title}. {desc}\" if desc else title\n",
        "        tokens = clean_text(f\"{title} {desc} {content}\")\n",
        "        source = art.get(\"source\", {})\n",
        "        source_name = source.get(\"name\", \"Unknown\") if isinstance(source, dict) else str(source or \"Unknown\")\n",
        "        processed.append({\n",
        "            \"id\": f\"article_{idx}\",\n",
        "            \"url\": art.get(\"url\", \"\"),\n",
        "            \"title\": title,\n",
        "            \"description\": desc,\n",
        "            \"text\": readable_text,\n",
        "            \"source\": source_name,\n",
        "            \"published\": art.get(\"publishedAt\", \"\"),\n",
        "            \"tokens\": tokens\n",
        "        })\n",
        "\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    out_path = f\"data/processed/corpus_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(processed, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"[OK] Saved {len(processed)} articles to {out_path}\")\n",
        "    return processed\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    preprocess()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o7OvN6u2Dhw",
        "outputId": "c46167b3-85e1-4113-9eab-69fc2f96dbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Create search_articles.py (with Topic Visualization)\n",
        "%%writefile search_articles.py\n",
        "import os, re, math, json\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import itertools\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return re.findall(r\"[a-zA-Z0-9']+\", text.lower()) if text else []\n",
        "\n",
        "def compute_tf(tokens):\n",
        "    if not tokens: return {}\n",
        "    counts = defaultdict(int)\n",
        "    for t in tokens: counts[t] += 1\n",
        "    return {t: c/len(tokens) for t,c in counts.items()}\n",
        "\n",
        "def compute_df(doc_tokens):\n",
        "    df = defaultdict(int)\n",
        "    for tokens in doc_tokens:\n",
        "        for t in set(tokens): df[t] += 1\n",
        "    return df\n",
        "\n",
        "def tfidf_vector(tokens, idf):\n",
        "    tf = compute_tf(tokens)\n",
        "    return {t: tf[t] * idf.get(t, 0) for t in tf}\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    if not a or not b: return 0.0\n",
        "    shared = set(a.keys()) & set(b.keys())\n",
        "    if not shared: return 0.0\n",
        "    dot = sum(a[k]*b[k] for k in shared)\n",
        "    na, nb = math.sqrt(sum(v*v for v in a.values())), math.sqrt(sum(v*v for v in b.values()))\n",
        "    return dot/(na*nb) if na and nb else 0.0\n",
        "\n",
        "def load_processed(processed_dir=\"data/processed\"):\n",
        "    files = sorted([f for f in os.listdir(processed_dir) if f.endswith(\".json\")], reverse=True)\n",
        "    if not files: raise FileNotFoundError(\"No processed files\")\n",
        "    with open(os.path.join(processed_dir, files[0]), \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def search_corpus(query: str, corpus: List[Dict], k: int = 5) -> List[Dict]:\n",
        "    if not query or not corpus: return []\n",
        "    query_lower = query.lower()\n",
        "    skip_words = {\"what\", \"are\", \"the\", \"latest\", \"tell\", \"me\", \"about\", \"is\", \"happening\", \"with\"}\n",
        "    query_keywords = [w for w in tokenize(query) if w not in skip_words]\n",
        "    if any(w in query_lower for w in [\"ai\", \"artificial intelligence\"]):\n",
        "        query_keywords = [\"ai\", \"artificial\", \"intelligence\"] + query_keywords\n",
        "    docs = []\n",
        "    for d in corpus:\n",
        "        tokens = d.get(\"tokens\", [])\n",
        "        title_tokens = tokenize(d.get(\"title\", \"\"))\n",
        "        docs.append({\"id\": d[\"id\"], \"title\": d.get(\"title\", \"\"), \"text\": d.get(\"text\", d.get(\"description\", \"\")),\n",
        "            \"source\": d.get(\"source\", \"Unknown\"), \"published\": d.get(\"published\", \"\"), \"url\": d.get(\"url\", \"\"),\n",
        "            \"tokens\": tokens + title_tokens * 3})\n",
        "    doc_tokens = [d[\"tokens\"] for d in docs]\n",
        "    df = compute_df(doc_tokens)\n",
        "    n = len(docs)\n",
        "    idf = {t: math.log((n+1)/(df[t]+1))+1 for t in df}\n",
        "    doc_vecs = [tfidf_vector(d[\"tokens\"], idf) for d in docs]\n",
        "    q_vec = tfidf_vector(query_keywords or tokenize(query), idf)\n",
        "    if not q_vec: return []\n",
        "    scored = sorted([(cosine_similarity(q_vec, dv), i) for i,dv in enumerate(doc_vecs)], reverse=True)\n",
        "    results = []\n",
        "    for score, i in scored:\n",
        "        if score > 0:\n",
        "            d = docs[i]\n",
        "            snippet = d.get(\"text\", \"\")[:250] if d.get(\"text\") else \"(no content)\"\n",
        "            results.append({\"id\": d[\"id\"], \"title\": d[\"title\"], \"score\": round(score, 4), \"snippet\": snippet,\n",
        "                \"source\": d[\"source\"], \"published\": d[\"published\"], \"url\": d[\"url\"]})\n",
        "        if len(results) >= k: break\n",
        "    return results\n",
        "\n",
        "# Dynamic Topic Extraction\n",
        "STOPWORDS = {\n",
        "    \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\",\n",
        "    \"by\", \"from\", \"as\", \"is\", \"was\", \"are\", \"were\", \"been\", \"be\", \"have\", \"has\", \"had\",\n",
        "    \"do\", \"does\", \"did\", \"will\", \"would\", \"could\", \"should\", \"may\", \"might\", \"must\",\n",
        "    \"it\", \"its\", \"this\", \"that\", \"these\", \"those\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"when\", \"where\", \"why\", \"how\", \"all\", \"each\", \"every\",\n",
        "    \"both\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\",\n",
        "    \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"just\", \"can\", \"now\", \"new\", \"said\",\n",
        "    \"says\", \"say\", \"also\", \"get\", \"got\", \"one\", \"two\", \"first\", \"last\", \"year\", \"years\",\n",
        "    \"time\", \"way\", \"even\", \"back\", \"after\", \"use\", \"make\", \"made\", \"want\", \"see\", \"look\",\n",
        "    \"into\", \"over\", \"out\", \"up\", \"down\", \"about\", \"through\", \"between\", \"under\", \"again\",\n",
        "    \"there\", \"here\", \"then\", \"once\", \"during\", \"before\", \"being\", \"any\", \"many\", \"much\",\n",
        "    \"while\", \"against\", \"part\", \"based\", \"using\", \"used\", \"according\", \"around\", \"well\",\n",
        "    \"going\", \"come\", \"take\", \"thing\", \"things\", \"really\", \"still\", \"since\", \"without\",\n",
        "    \"need\", \"needs\", \"like\", \"know\", \"think\", \"work\", \"working\", \"works\", \"people\",\n",
        "    \"company\", \"companies\", \"percent\", \"million\", \"billion\", \"including\", \"however\",\n",
        "    \"though\", \"another\", \"something\", \"nothing\", \"everything\", \"anything\", \"someone\",\n",
        "    \"everyone\", \"today\", \"week\", \"month\", \"day\", \"days\", \"weeks\", \"months\", \"report\",\n",
        "    \"reported\", \"reports\", \"news\", \"article\", \"story\", \"via\", \"per\", \"etc\", \"ie\", \"eg\",\n",
        "    \"read\", \"best\", \"good\", \"better\", \"available\", \"right\", \"left\", \"big\", \"small\",\n",
        "    \"long\", \"short\", \"high\", \"low\", \"old\", \"young\", \"early\", \"late\", \"next\", \"last\"\n",
        "}\n",
        "\n",
        "def extract_ngrams(tokens: List[str], n: int = 2) -> List[str]:\n",
        "    if len(tokens) < n: return []\n",
        "    return [\" \".join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "\n",
        "def get_document_terms(doc: Dict) -> List[str]:\n",
        "    # Use the pre-processed tokens and title from the corpus\n",
        "    title_tokens = tokenize(doc.get(\"title\", \"\"))\n",
        "    text_tokens = tokenize(doc.get(\"text\", \"\"))\n",
        "    content_tokens = doc.get(\"tokens\", [])\n",
        "\n",
        "    title_filtered = [t for t in title_tokens if t not in STOPWORDS and len(t) > 2]\n",
        "    text_filtered = [t for t in text_tokens if t not in STOPWORDS and len(t) > 2]\n",
        "    content_filtered = [t for t in content_tokens if t not in STOPWORDS and len(t) > 2]\n",
        "\n",
        "    # Weight title terms more heavily\n",
        "    terms = title_filtered * 5 + text_filtered * 2 + content_filtered\n",
        "    title_bigrams = extract_ngrams(title_filtered, 2)\n",
        "    return terms + title_bigrams\n",
        "\n",
        "def extract_top_terms(corpus: List[Dict], top_n: int = 50) -> List[Tuple[str, float]]:\n",
        "    doc_terms = [get_document_terms(doc) for doc in corpus]\n",
        "    df = defaultdict(int)\n",
        "    for terms in doc_terms:\n",
        "        for term in set(terms): df[term] += 1\n",
        "    n_docs = len(corpus)\n",
        "    term_scores = defaultdict(float)\n",
        "    for terms in doc_terms:\n",
        "        tf = Counter(terms)\n",
        "        max_tf = max(tf.values()) if tf else 1\n",
        "        for term, count in tf.items():\n",
        "            tfidf = (count / max_tf) * math.log((n_docs + 1) / (df[term] + 1))\n",
        "            term_scores[term] += tfidf\n",
        "    sorted_terms = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    min_docs, max_docs = max(2, n_docs * 0.02), n_docs * 0.7\n",
        "    filtered = [(t, s) for t, s in sorted_terms if min_docs <= df[t] <= max_docs and len(t) > 2]\n",
        "    return filtered[:top_n]\n",
        "\n",
        "def cluster_terms_into_topics(corpus: List[Dict], n_topics: int = 5) -> List[Dict]:\n",
        "    top_terms = extract_top_terms(corpus, top_n=100)\n",
        "    term_set = {t for t, _ in top_terms}\n",
        "    cooccurrence = defaultdict(lambda: defaultdict(int))\n",
        "    term_to_docs = defaultdict(set)\n",
        "    for doc_idx, doc in enumerate(corpus):\n",
        "        doc_terms = set(get_document_terms(doc)) & term_set\n",
        "        for term in doc_terms: term_to_docs[term].add(doc_idx)\n",
        "        for t1, t2 in itertools.combinations(list(doc_terms), 2):\n",
        "            cooccurrence[t1][t2] += 1\n",
        "            cooccurrence[t2][t1] += 1\n",
        "    used_terms, topics = set(), []\n",
        "    for seed_term, seed_score in top_terms:\n",
        "        if seed_term in used_terms: continue\n",
        "        related = [(seed_term, seed_score)]\n",
        "        candidates = sorted(cooccurrence[seed_term].items(), key=lambda x: x[1], reverse=True)\n",
        "        for term, cooc_count in candidates:\n",
        "            if term not in used_terms and len(related) < 8 and cooc_count >= 2:\n",
        "                related.append((term, cooc_count))\n",
        "        if len(related) >= 2:\n",
        "            topic_terms = [t for t, _ in related]\n",
        "            used_terms.update(topic_terms)\n",
        "            topic_docs = set()\n",
        "            for term in topic_terms[:3]: topic_docs.update(term_to_docs[term])\n",
        "            name_parts = [t.title() for t, _ in related[:2]]\n",
        "            topic_name = \" & \".join(name_parts)\n",
        "            # Get actual article titles for this topic\n",
        "            sample_articles = [corpus[idx].get(\"title\", \"Unknown\") for idx in list(topic_docs)[:3]]\n",
        "            topics.append({\"name\": topic_name, \"terms\": topic_terms, \"count\": len(topic_docs),\n",
        "                \"doc_indices\": topic_docs, \"sample_articles\": sample_articles})\n",
        "        if len(topics) >= n_topics: break\n",
        "    topics.sort(key=lambda x: x[\"count\"], reverse=True)\n",
        "    return topics[:n_topics]\n",
        "\n",
        "def extract_subtrends(corpus: List[Dict], topic: Dict, n_subtrends: int = 5) -> List[Dict]:\n",
        "    topic_docs = [corpus[i] for i in topic[\"doc_indices\"] if i < len(corpus)]\n",
        "    if not topic_docs: return []\n",
        "    main_terms = set(topic[\"terms\"])\n",
        "    term_counts, term_docs = Counter(), defaultdict(set)\n",
        "    for doc_idx, doc in enumerate(topic_docs):\n",
        "        for term in get_document_terms(doc):\n",
        "            if term not in main_terms and term not in STOPWORDS and len(term) > 2:\n",
        "                term_counts[term] += 1\n",
        "                term_docs[term].add(doc_idx)\n",
        "    min_docs, max_docs = max(1, len(topic_docs) * 0.1), len(topic_docs) * 0.8\n",
        "    subtrend_candidates = [(t, c) for t, c in term_counts.most_common(50) if min_docs <= len(term_docs[t]) <= max_docs]\n",
        "    subtrends, used = [], set()\n",
        "    for term, count in subtrend_candidates:\n",
        "        if term in used: continue\n",
        "        relevance = len(term_docs[term]) / len(topic_docs)\n",
        "        subtrends.append({\"name\": term.title(), \"relevance\": round(min(0.95, relevance + 0.2), 2),\n",
        "            \"article_count\": len(term_docs[term])})\n",
        "        used.add(term)\n",
        "        if len(subtrends) >= n_subtrends: break\n",
        "    if len(subtrends) < n_subtrends:\n",
        "        for term, count in term_counts.most_common(20):\n",
        "            if term not in used and len(subtrends) < n_subtrends:\n",
        "                subtrends.append({\"name\": term.title(), \"relevance\": round(0.3 + (count / len(topic_docs)) * 0.4, 2),\n",
        "                    \"article_count\": count})\n",
        "    return sorted(subtrends, key=lambda x: x[\"relevance\"], reverse=True)[:n_subtrends]\n",
        "\n",
        "def get_topic_visualization_data(corpus: List[Dict]) -> Dict[str, Any]:\n",
        "    topics = cluster_terms_into_topics(corpus, n_topics=5)\n",
        "    viz_data = {\"topics\": [], \"subtrends\": {}}\n",
        "    for topic in topics:\n",
        "        topic_name = topic[\"name\"]\n",
        "        viz_data[\"topics\"].append({\"name\": topic_name, \"count\": topic[\"count\"],\n",
        "            \"sample_articles\": topic[\"sample_articles\"], \"key_terms\": topic[\"terms\"][:5]})\n",
        "        subtrends = extract_subtrends(corpus, topic, n_subtrends=5)\n",
        "        viz_data[\"subtrends\"][topic_name] = subtrends\n",
        "    return viz_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq_ZUTkm2Gry",
        "outputId": "a52f115a-2aa3-41e0-a3af-f6d98a0ee195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting search_articles.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===CELL 6: Create llm_interface.py===\n",
        "#@title 6. Create llm_interface.py\n",
        "%%writefile llm_interface.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "class HF_LLM:\n",
        "    def __init__(self, model_name=MODEL_NAME, max_new_tokens=256):\n",
        "        self.model_name = model_name\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "            self.device = \"cuda\"\n",
        "            self.dtype = torch.float16\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No GPU, using CPU\")\n",
        "            self.device = \"cpu\"\n",
        "            self.dtype = torch.float32\n",
        "\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, device_map=\"auto\" if self.device == \"cuda\" else None,\n",
        "            torch_dtype=self.dtype, trust_remote_code=True\n",
        "        )\n",
        "        self.gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, temperature=0.7, do_sample=True)\n",
        "        print(f\"‚úÖ Model loaded on {next(self.model.parameters()).device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmUn0QfR2Kbe",
        "outputId": "a3643971-4266-4420-a674-f24d0e584a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting llm_interface.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7. Create app.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import re, torch, os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from fetch_news import fetch_news_articles, get_date_range_options\n",
        "from preprocess import preprocess\n",
        "from search_articles import load_processed, search_corpus, get_topic_visualization_data\n",
        "from llm_interface import HF_LLM\n",
        "\n",
        "st.set_page_config(page_title=\"Tech Trends Agent\", layout=\"wide\")\n",
        "\n",
        "NEWSAPI_KEY = \"821aeebd32f34b238e99e1282b9eb935\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "    return HF_LLM()\n",
        "\n",
        "\n",
        "def get_viz_data(_corpus):\n",
        "    return get_topic_visualization_data(_corpus)\n",
        "\n",
        "def generate_answer(llm, question, results):\n",
        "    context = \"\"\n",
        "    for r in results[:5]:\n",
        "        context += f\"- {r['title']}: {r['snippet'][:200]}\\n\\n\"\n",
        "    prompt = f\"\"\"Based on these recent tech news articles:\n",
        "\n",
        "{context}\n",
        "\n",
        "Write a 3-4 sentence paragraph answering: {question}\n",
        "\n",
        "Be specific and mention actual details from the articles. Write naturally.\n",
        "\n",
        "Answer:\"\"\"\n",
        "    inputs = llm.tokenizer(prompt, return_tensors=\"pt\").to(llm.model.device)\n",
        "    with torch.no_grad():\n",
        "        output = llm.model.generate(\n",
        "            **inputs, max_new_tokens=200, temperature=0.7,\n",
        "            do_sample=True, pad_token_id=llm.tokenizer.eos_token_id\n",
        "        )\n",
        "    answer = llm.tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "    answer = answer.split('\\n\\n')[0].strip()\n",
        "    return re.sub(r'^Answer:\\s*', '', answer)\n",
        "\n",
        "def format_date(date_str):\n",
        "    if not date_str: return \"\"\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%b %d, %Y\")\n",
        "    except: return \"\"\n",
        "\n",
        "def process_question(question, llm, corpus):\n",
        "    results = search_corpus(question, corpus, k=5)\n",
        "    answer = generate_answer(llm, question, results)\n",
        "    return answer, results\n",
        "\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "if 'corpus' not in st.session_state:\n",
        "    st.session_state.corpus = None\n",
        "if 'data_loaded' not in st.session_state:\n",
        "    st.session_state.data_loaded = False\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Data Source\")\n",
        "    st.success(\"API Key configured\")\n",
        "    st.divider()\n",
        "    st.subheader(\"Date Range\")\n",
        "    date_options = get_date_range_options()\n",
        "    range_choice = st.selectbox(\"Quick Select\", [\"Custom\"] + list(date_options.keys()))\n",
        "    if range_choice == \"Custom\":\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            from_date = st.date_input(\"From\", datetime.now() - timedelta(days=7))\n",
        "        with col2:\n",
        "            to_date = st.date_input(\"To\", datetime.now())\n",
        "    else:\n",
        "        from_date, to_date = date_options[range_choice]\n",
        "        from_date = from_date.date() if hasattr(from_date, 'date') else from_date\n",
        "        to_date = to_date.date() if hasattr(to_date, 'date') else to_date\n",
        "        st.info(f\"{from_date} to {to_date}\")\n",
        "    max_articles = st.slider(\"Max Articles\", 10, 100, 100, step=10)\n",
        "    st.divider()\n",
        "    if st.button(\"Fetch Articles\", type=\"primary\", use_container_width=True):\n",
        "        with st.spinner(\"Fetching articles...\"):\n",
        "            try:\n",
        "                articles = fetch_news_articles(api_key=NEWSAPI_KEY, from_date=str(from_date), to_date=str(to_date), max_articles=max_articles)\n",
        "                if articles:\n",
        "                    with st.spinner(\"Processing...\"):\n",
        "                        st.session_state.corpus = preprocess(articles)\n",
        "                        st.session_state.data_loaded = True\n",
        "                        st.session_state.chat_history = []\n",
        "                        st.cache_data.clear()\n",
        "                    st.success(f\"Loaded {len(st.session_state.corpus)} articles!\")\n",
        "                else:\n",
        "                    st.warning(\"No articles found\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {str(e)}\")\n",
        "    if not st.session_state.data_loaded:\n",
        "        try:\n",
        "            st.session_state.corpus = load_processed()\n",
        "            st.session_state.data_loaded = True\n",
        "            st.info(f\"Loaded {len(st.session_state.corpus)} cached articles\")\n",
        "        except:\n",
        "            pass\n",
        "    st.divider()\n",
        "    if st.button(\"Clear Chat\", use_container_width=True):\n",
        "        st.session_state.chat_history = []\n",
        "        st.rerun()\n",
        "    if st.button(\"Re-analyze Topics\", use_container_width=True):\n",
        "        st.cache_data.clear()\n",
        "        st.rerun()\n",
        "\n",
        "st.title(\"Tech Trends News Agent\")\n",
        "\n",
        "if not st.session_state.data_loaded or st.session_state.corpus is None:\n",
        "    st.warning(\"No articles loaded. Please fetch articles from the sidebar.\")\n",
        "else:\n",
        "    llm = load_llm()\n",
        "    corpus = st.session_state.corpus\n",
        "    tab1, tab2 = st.tabs([\"Chat\", \"Topic Visualization\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.caption(f\"Ask questions about {len(corpus)} recent tech news articles\")\n",
        "\n",
        "        # Chat container with fixed height for scrolling\n",
        "        chat_container = st.container(height=500)\n",
        "\n",
        "        with chat_container:\n",
        "            if not st.session_state.chat_history:\n",
        "                st.markdown(\"### Try asking:\")\n",
        "                col1, col2 = st.columns(2)\n",
        "                suggestions = [\"What are the latest AI trends?\", \"Tell me about tech news\", \"What are the biggest tech stories?\", \"Any news about smartphones?\"]\n",
        "                for i, sug in enumerate(suggestions):\n",
        "                    with (col1 if i % 2 == 0 else col2):\n",
        "                        if st.button(sug, key=f\"sug_{i}\", use_container_width=True):\n",
        "                            st.session_state.pending_question = sug\n",
        "                            st.rerun()\n",
        "            else:\n",
        "                for msg in st.session_state.chat_history:\n",
        "                    with st.chat_message(msg[\"role\"]):\n",
        "                        st.write(msg[\"content\"])\n",
        "                        if msg[\"role\"] == \"assistant\" and \"sources\" in msg:\n",
        "                            with st.expander(\"View Sources\"):\n",
        "                                for s in msg[\"sources\"]:\n",
        "                                    st.markdown(f\"**[{s['title']}]({s.get('url', '#')})**\")\n",
        "                                    st.caption(f\"{s.get('source', 'Unknown')} | {format_date(s.get('published', ''))} | Score: {s['score']:.1%}\")\n",
        "\n",
        "        # Process pending question from button click\n",
        "        if 'pending_question' in st.session_state:\n",
        "            question = st.session_state.pending_question\n",
        "            del st.session_state.pending_question\n",
        "            st.session_state.chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "            with st.spinner(\"Searching...\"):\n",
        "                answer, results = process_question(question, llm, corpus)\n",
        "            st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer, \"sources\": results})\n",
        "            st.rerun()\n",
        "\n",
        "        # Chat input always at bottom\n",
        "        if prompt := st.chat_input(\"Ask about technology trends...\"):\n",
        "            st.session_state.chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "            with st.spinner(\"Searching...\"):\n",
        "                answer, results = process_question(prompt, llm, corpus)\n",
        "            st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer, \"sources\": results})\n",
        "            st.rerun()\n",
        "\n",
        "    with tab2:\n",
        "        st.header(\"Discovered Topics & Sub-Trends\")\n",
        "        viz_data = get_viz_data(corpus)\n",
        "        if not viz_data[\"topics\"]:\n",
        "            st.warning(\"No topics found.\")\n",
        "        else:\n",
        "            st.subheader(\"Top 5 Topics\")\n",
        "            topic_df = pd.DataFrame({\"Topic\": [t[\"name\"] for t in viz_data[\"topics\"]], \"Articles\": [t[\"count\"] for t in viz_data[\"topics\"]]})\n",
        "            st.bar_chart(topic_df.set_index(\"Topic\"))\n",
        "            st.subheader(\"Topic Deep Dive\")\n",
        "            for i, topic_info in enumerate(viz_data[\"topics\"]):\n",
        "                topic_name = topic_info[\"name\"]\n",
        "                subtrends = viz_data[\"subtrends\"].get(topic_name, [])\n",
        "                with st.expander(f\"{i+1}. {topic_name} ({topic_info['count']} articles)\", expanded=(i==0)):\n",
        "                    if topic_info.get(\"key_terms\"):\n",
        "                        st.markdown(\"**Key Terms:** \" + \" \".join([f\"`{t}`\" for t in topic_info[\"key_terms\"]]))\n",
        "                    col1, col2 = st.columns(2)\n",
        "                    with col1:\n",
        "                        st.markdown(\"**Sample Articles:**\")\n",
        "                        for j, title in enumerate(topic_info.get(\"sample_articles\", [])[:3], 1):\n",
        "                            st.markdown(f\"{j}. {title[:60]}...\")\n",
        "                    with col2:\n",
        "                        st.markdown(\"**Sub-Trends:**\")\n",
        "                        for st_info in subtrends[:5]:\n",
        "                            st.markdown(f\"**{st_info['name']}**\")\n",
        "                            st.progress(st_info[\"relevance\"])\n",
        "            st.divider()\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            total_cat = sum(t[\"count\"] for t in viz_data[\"topics\"])\n",
        "            col1.metric(\"Total Articles\", len(corpus))\n",
        "            col2.metric(\"Categorized\", total_cat)\n",
        "            col3.metric(\"Coverage\", f\"{total_cat/len(corpus)*100:.0f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alj-1QTRMEP6",
        "outputId": "09d8608d-7291-4d71-b27e-e45ed92da330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===CELL 8: Launch the App===\n",
        "#@title 8. Launch Streamlit App\n",
        "!pip install -q pyngrok\n",
        "!pkill -f streamlit\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "ngrok.set_auth_token(\"36aLZAaAh8rvnvKvJEyLtC6HM3Y_6hwzr4BhX2FZVwVLSPgC8\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"=\"*60)\n",
        "print(f\"üåê OPEN THIS URL: {public_url}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚è≥ Wait ~60 seconds for model to load, then use the app!\")\n",
        "\n",
        "!streamlit run app.py --server.port 8501 --server.headless true &"
      ],
      "metadata": {
        "id": "QlyMoLx12O6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaa738e-5587-4547-f37d-76d920487b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üåê OPEN THIS URL: NgrokTunnel: \"https://subaqueous-unusably-oralee.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "============================================================\n",
            "\n",
            "‚è≥ Wait ~60 seconds for model to load, then use the app!\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-12-09T01:01:24+0000 lvl=warn msg=\"failed to open private leg\" id=347fc994fc14 privaddr=localhost:8501 err=\"dial tcp [::1]:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-12-09T01:01:24+0000 lvl=warn msg=\"failed to open private leg\" id=08246e427f27 privaddr=localhost:8501 err=\"dial tcp [::1]:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-12-09T01:01:24+0000 lvl=warn msg=\"failed to open private leg\" id=179254b8a06e privaddr=localhost:8501 err=\"dial tcp [::1]:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-12-09T01:01:26+0000 lvl=warn msg=\"failed to open private leg\" id=111ef9b39a38 privaddr=localhost:8501 err=\"dial tcp [::1]:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-12-09T01:01:26+0000 lvl=warn msg=\"failed to open private leg\" id=9d2ae0dddf49 privaddr=localhost:8501 err=\"dial tcp [::1]:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-12-09T01:01:26+0000 lvl=warn msg=\"failed to open private leg\" id=25198d03078b privaddr=localhost:8501 err=\"dial tcp [::1]:8501: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.83.185.180:8501\u001b[0m\n",
            "\u001b[0m\n",
            "Fetching articles from 2025-12-02 to 2025-12-09...\n",
            "  Page 1: found 98 articles\n",
            "  Total available: 5649\n",
            "Error fetching page 2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "[OK] Saved 98 articles to data/raw/newsapi_20251209_010140.json\n",
            "Processing 98 articles...\n",
            "[OK] Saved 98 articles to data/processed/corpus_20251209_010143.json\n",
            "‚úÖ GPU: Tesla T4\n",
            "Loading Qwen/Qwen2.5-1.5B-Instruct...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-12-09 01:01:44.751113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765242104.769044   12996 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765242104.774065   12996 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765242104.787850   12996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765242104.787874   12996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765242104.787878   12996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765242104.787882   12996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 01:01:44.793374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "‚úÖ Model loaded on cuda:0\n",
            "Fetching articles from 2025-12-02 to 2025-12-09...\n",
            "  Page 1: found 98 articles\n",
            "  Total available: 5649\n",
            "Error fetching page 2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "[OK] Saved 98 articles to data/raw/newsapi_20251209_010438.json\n",
            "Processing 98 articles...\n",
            "[OK] Saved 98 articles to data/processed/corpus_20251209_010438.json\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}